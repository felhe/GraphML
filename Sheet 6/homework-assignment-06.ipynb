{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "J_m9l6OYCQZP",
    "ExecuteTime": {
     "end_time": "2024-07-08T17:56:20.051383Z",
     "start_time": "2024-07-08T17:56:20.020719Z"
    }
   },
   "source": [
    "import os\n",
    "import torch\n",
    "import torch_geometric\n",
    "torch_geometric.__version__"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.5.3'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZoXlf4MtYrbz"
   },
   "source": [
    "# 1.1 Implementation of GraphSAGE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H4ne6Gw-CT5G"
   },
   "source": [
    "## GNN Stack Module\n",
    "\n",
    "Below is the implementation of a general GNN stack, a plugin for GNN layer. Your implementation of the **GraphSage** and **GAT** layer will function as a component in the GNNStack Module."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Ys8vZAFPCWWe",
    "ExecuteTime": {
     "end_time": "2024-07-08T17:56:52.304599Z",
     "start_time": "2024-07-08T17:56:52.186658Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch_scatter\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch_geometric.nn as pyg_nn\n",
    "import torch_geometric.utils as pyg_utils\n",
    "\n",
    "from torch import Tensor\n",
    "from typing import Union, Tuple, Optional\n",
    "from torch_geometric.typing import (OptPairTensor, Adj, Size, NoneType,\n",
    "                                    OptTensor)\n",
    "\n",
    "from torch.nn import Parameter, Linear\n",
    "from torch_sparse import SparseTensor, set_diag\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.utils import remove_self_loops, add_self_loops, softmax\n",
    "\n",
    "class GNNStack(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, args, emb=False):\n",
    "        super(GNNStack, self).__init__()\n",
    "        conv_model = self.build_conv_model(args.model_type)\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.convs.append(conv_model(input_dim, hidden_dim))\n",
    "        assert (args.num_layers >= 1), 'Number of layers is not >=1'\n",
    "        for l in range(args.num_layers-1):\n",
    "            self.convs.append(conv_model(args.heads * hidden_dim, hidden_dim))\n",
    "\n",
    "        # post-message-passing\n",
    "        self.post_mp = nn.Sequential(\n",
    "            nn.Linear(args.heads * hidden_dim, hidden_dim), nn.Dropout(args.dropout),\n",
    "            nn.Linear(hidden_dim, output_dim))\n",
    "\n",
    "        self.dropout = args.dropout\n",
    "        self.num_layers = args.num_layers\n",
    "\n",
    "        self.emb = emb\n",
    "\n",
    "    def build_conv_model(self, model_type):\n",
    "        if model_type == 'GraphSage':\n",
    "            return GraphSage\n",
    "        elif model_type == 'GAT':\n",
    "            return GAT\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.convs[i](x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout,training=self.training)\n",
    "\n",
    "        x = self.post_mp(x)\n",
    "\n",
    "        if self.emb == True:\n",
    "            return x\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "    def loss(self, pred, label):\n",
    "        return F.nll_loss(pred, label)"
   ],
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "/home/felix/.conda/envs/graphml/lib/python3.10/site-packages/torch_scatter/_version_cpu.so: undefined symbol: __cxa_call_terminate",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch_scatter\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnn\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnn\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfunctional\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mF\u001B[39;00m\n",
      "File \u001B[0;32m~/.conda/envs/graphml/lib/python3.10/site-packages/torch_scatter/__init__.py:16\u001B[0m\n\u001B[1;32m     14\u001B[0m spec \u001B[38;5;241m=\u001B[39m cuda_spec \u001B[38;5;129;01mor\u001B[39;00m cpu_spec\n\u001B[1;32m     15\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m spec \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m---> 16\u001B[0m     \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mops\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_library\u001B[49m\u001B[43m(\u001B[49m\u001B[43mspec\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43morigin\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m os\u001B[38;5;241m.\u001B[39mgetenv(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mBUILD_DOCS\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m0\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m1\u001B[39m\u001B[38;5;124m'\u001B[39m:  \u001B[38;5;66;03m# pragma: no cover\u001B[39;00m\n\u001B[1;32m     18\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCould not find module \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlibrary\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m_cpu\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m in \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     19\u001B[0m                       \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mosp\u001B[38;5;241m.\u001B[39mdirname(\u001B[38;5;18m__file__\u001B[39m)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/.conda/envs/graphml/lib/python3.10/site-packages/torch/_ops.py:1032\u001B[0m, in \u001B[0;36m_Ops.load_library\u001B[0;34m(self, path)\u001B[0m\n\u001B[1;32m   1027\u001B[0m path \u001B[38;5;241m=\u001B[39m _utils_internal\u001B[38;5;241m.\u001B[39mresolve_library_path(path)\n\u001B[1;32m   1028\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m dl_open_guard():\n\u001B[1;32m   1029\u001B[0m     \u001B[38;5;66;03m# Import the shared library into the process, thus running its\u001B[39;00m\n\u001B[1;32m   1030\u001B[0m     \u001B[38;5;66;03m# static (global) initialization code in order to register custom\u001B[39;00m\n\u001B[1;32m   1031\u001B[0m     \u001B[38;5;66;03m# operators with the JIT.\u001B[39;00m\n\u001B[0;32m-> 1032\u001B[0m     \u001B[43mctypes\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mCDLL\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1033\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mloaded_libraries\u001B[38;5;241m.\u001B[39madd(path)\n",
      "File \u001B[0;32m~/.conda/envs/graphml/lib/python3.10/ctypes/__init__.py:374\u001B[0m, in \u001B[0;36mCDLL.__init__\u001B[0;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001B[0m\n\u001B[1;32m    371\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_FuncPtr \u001B[38;5;241m=\u001B[39m _FuncPtr\n\u001B[1;32m    373\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m handle \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 374\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle \u001B[38;5;241m=\u001B[39m \u001B[43m_dlopen\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    375\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    376\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle \u001B[38;5;241m=\u001B[39m handle\n",
      "\u001B[0;31mOSError\u001B[0m: /home/felix/.conda/envs/graphml/lib/python3.10/site-packages/torch_scatter/_version_cpu.so: undefined symbol: __cxa_call_terminate"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "syDtxjxoCZgq"
   },
   "source": [
    "### GraphSage Implementation\n",
    "\n",
    "For a given *central* node $v$ with current embedding $h_v^{l-1}$, the message passing update rule to tranform $h_v^{l-1} \\rightarrow h_v^l$ is as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "h_v^{(l)} = W_l\\cdot h_v^{(l-1)} + W_r \\cdot AGG(\\{h_u^{(l-1)}, \\forall u \\in N(v) \\})\n",
    "\\end{equation}\n",
    "\n",
    "where $W_1$ and $W_2$ are learanble weight matrices and the nodes $u$ are *neighboring* nodes. Additionally, we use mean aggregation for simplicity:\n",
    "\n",
    "\\begin{equation}\n",
    "AGG(\\{h_u^{(l-1)}, \\forall u \\in N(v) \\}) = \\frac{1}{|N(v)|} \\sum_{u\\in N(v)} h_u^{(l-1)}\n",
    "\\end{equation}\n",
    "\n",
    "One thing to note is that we're adding a **skip connection** to our GraphSage implementation through the term $W_l\\cdot h_v^{(l-1)}$.\n",
    "\n",
    "Lastly, $\\ell$-2 normalization of the node embeddings is applied after each iteration."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "RwG4HqCFCaOD",
    "ExecuteTime": {
     "end_time": "2024-07-08T17:57:01.713765Z",
     "start_time": "2024-07-08T17:57:01.581025Z"
    }
   },
   "source": [
    "from torch_scatter import scatter\n",
    "\n",
    "# TODO: Change `__init__`, `forward`, `message`, `aggregate` and `update`\n",
    "class GraphSage(MessagePassing):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, normalize = True,\n",
    "                 bias = False, **kwargs):\n",
    "        super(GraphSage, self).__init__(**kwargs)\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.normalize = normalize\n",
    "\n",
    "        ############# Your code here #############\n",
    "        # Define the (PyTorch) layers needed for the message and update functions below.\n",
    "        # self.lin_l is the linear transformation that you apply to embedding\n",
    "        #            for central node.\n",
    "        # self.lin_r is the linear transformation that you apply to aggregated\n",
    "        #            message from neighbors.\n",
    "\n",
    "        self.lin_l = None\n",
    "        self.lin_r = None\n",
    "\n",
    "        ############################################################################\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.lin_l.reset_parameters()\n",
    "        self.lin_r.reset_parameters()\n",
    "\n",
    "    def forward(self, x, edge_index, size = None):\n",
    "        \"\"\"\"\"\"\n",
    " \n",
    "        out = None\n",
    "\n",
    "        ############# Your code here #############\n",
    "        # Implement message passing, as well as any post-processing.\n",
    "        # 1. Call the propagate function to conduct the message passing.\n",
    "        #    1.1 See the description of propagate above or the following link for more information:\n",
    "        #        https://pytorch-geometric.readthedocs.io/en/latest/notes/create_gnn.html\n",
    "        #    1.2 We will only use the representation for neighbor nodes (x_j), so by default\n",
    "        #        we pass the same representation for central and neighbor nodes as x=(x, x).\n",
    "        # 2. Post-processing: Optional l2-normalization\n",
    "\n",
    "        out = self.propagate(edge_index, x=(x, x))\n",
    "\n",
    "        if self.normalize:\n",
    "            out = F.normalize(out)\n",
    "        \n",
    "        ############################################################################\n",
    "\n",
    "        return out\n",
    "\n",
    "    def message(self, x_j):\n",
    "\n",
    "        ############# Your code here #############\n",
    "        # Implement your message function here.\n",
    "        # Hint: Look at the formulation of the mean aggregation function, focusing on\n",
    "        # what message each neighboring node passes.\n",
    "\n",
    "        out = None\n",
    "        \n",
    "        ############################################################################\n",
    "        \n",
    "        return out\n",
    "\n",
    "    def aggregate(self, inputs, index, dim_size = None):\n",
    "\n",
    "        # The axis along which to index number of nodes.\n",
    "        node_dim = self.node_dim\n",
    "\n",
    "        ############# Your code here #############\n",
    "        # Implement your aggregate function here.\n",
    "        # See here as how to use torch_scatter.scatter:\n",
    "        # https://pytorch-scatter.readthedocs.io/en/latest/functions/scatter.html#torch_scatter.scatter\n",
    "\n",
    "        out = None\n",
    "\n",
    "        ############################################################################\n",
    "\n",
    "        return out\n",
    "\n",
    "    def update(self, inputs, x):\n",
    "\n",
    "        ############# Your code here #############\n",
    "        # Implement your update function here.\n",
    "\n",
    "        out = None\n",
    "        \n",
    "        return out\n"
   ],
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "/home/felix/.conda/envs/graphml/lib/python3.10/site-packages/torch_scatter/_version_cpu.so: undefined symbol: __cxa_call_terminate",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[7], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch_scatter\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m scatter\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# TODO: Change `__init__`, `forward`, `message`, `aggregate` and `update`\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mGraphSage\u001B[39;00m(MessagePassing):\n",
      "File \u001B[0;32m~/.conda/envs/graphml/lib/python3.10/site-packages/torch_scatter/__init__.py:16\u001B[0m\n\u001B[1;32m     14\u001B[0m spec \u001B[38;5;241m=\u001B[39m cuda_spec \u001B[38;5;129;01mor\u001B[39;00m cpu_spec\n\u001B[1;32m     15\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m spec \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m---> 16\u001B[0m     \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mops\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_library\u001B[49m\u001B[43m(\u001B[49m\u001B[43mspec\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43morigin\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m os\u001B[38;5;241m.\u001B[39mgetenv(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mBUILD_DOCS\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m0\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m1\u001B[39m\u001B[38;5;124m'\u001B[39m:  \u001B[38;5;66;03m# pragma: no cover\u001B[39;00m\n\u001B[1;32m     18\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCould not find module \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlibrary\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m_cpu\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m in \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     19\u001B[0m                       \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mosp\u001B[38;5;241m.\u001B[39mdirname(\u001B[38;5;18m__file__\u001B[39m)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/.conda/envs/graphml/lib/python3.10/site-packages/torch/_ops.py:1032\u001B[0m, in \u001B[0;36m_Ops.load_library\u001B[0;34m(self, path)\u001B[0m\n\u001B[1;32m   1027\u001B[0m path \u001B[38;5;241m=\u001B[39m _utils_internal\u001B[38;5;241m.\u001B[39mresolve_library_path(path)\n\u001B[1;32m   1028\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m dl_open_guard():\n\u001B[1;32m   1029\u001B[0m     \u001B[38;5;66;03m# Import the shared library into the process, thus running its\u001B[39;00m\n\u001B[1;32m   1030\u001B[0m     \u001B[38;5;66;03m# static (global) initialization code in order to register custom\u001B[39;00m\n\u001B[1;32m   1031\u001B[0m     \u001B[38;5;66;03m# operators with the JIT.\u001B[39;00m\n\u001B[0;32m-> 1032\u001B[0m     \u001B[43mctypes\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mCDLL\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1033\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mloaded_libraries\u001B[38;5;241m.\u001B[39madd(path)\n",
      "File \u001B[0;32m~/.conda/envs/graphml/lib/python3.10/ctypes/__init__.py:374\u001B[0m, in \u001B[0;36mCDLL.__init__\u001B[0;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001B[0m\n\u001B[1;32m    371\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_FuncPtr \u001B[38;5;241m=\u001B[39m _FuncPtr\n\u001B[1;32m    373\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m handle \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 374\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle \u001B[38;5;241m=\u001B[39m \u001B[43m_dlopen\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    375\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    376\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle \u001B[38;5;241m=\u001B[39m handle\n",
      "\u001B[0;31mOSError\u001B[0m: /home/felix/.conda/envs/graphml/lib/python3.10/site-packages/torch_scatter/_version_cpu.so: undefined symbol: __cxa_call_terminate"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V2dkgSuWCheU"
   },
   "source": [
    "## Building Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f_TIQ8NPCjBP"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def build_optimizer(args, params):\n",
    "    weight_decay = args.weight_decay\n",
    "    filter_fn = filter(lambda p : p.requires_grad, params)\n",
    "    if args.opt == 'adam':\n",
    "        optimizer = optim.Adam(filter_fn, lr=args.lr, weight_decay=weight_decay)\n",
    "    elif args.opt == 'sgd':\n",
    "        optimizer = optim.SGD(filter_fn, lr=args.lr, momentum=0.95, weight_decay=weight_decay)\n",
    "    elif args.opt == 'rmsprop':\n",
    "        optimizer = optim.RMSprop(filter_fn, lr=args.lr, weight_decay=weight_decay)\n",
    "    elif args.opt == 'adagrad':\n",
    "        optimizer = optim.Adagrad(filter_fn, lr=args.lr, weight_decay=weight_decay)\n",
    "    if args.opt_scheduler == 'none':\n",
    "        return None, optimizer\n",
    "    elif args.opt_scheduler == 'step':\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=args.opt_decay_step, gamma=args.opt_decay_rate)\n",
    "    elif args.opt_scheduler == 'cos':\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.opt_restart)\n",
    "    return scheduler, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hBYdWFwYCkwY"
   },
   "source": [
    "### Training and Testing Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_tZMWRc8CmGg"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from tqdm import trange\n",
    "import pandas as pd\n",
    "import copy\n",
    "\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "import torch_geometric.nn as pyg_nn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def train(dataset, args):\n",
    "\n",
    "    print(\"Node task. test set size:\", np.sum(dataset[0]['test_mask'].numpy()))\n",
    "    print()\n",
    "    test_loader = loader = DataLoader(dataset, batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "    # build model\n",
    "    model = GNNStack(dataset.num_node_features, args.hidden_dim, dataset.num_classes,\n",
    "                            args)\n",
    "    scheduler, opt = build_optimizer(args, model.parameters())\n",
    "\n",
    "    # train\n",
    "    losses = []\n",
    "    test_accs = []\n",
    "    best_acc = 0\n",
    "    best_model = None\n",
    "    for epoch in trange(args.epochs, desc=\"Training\", unit=\"Epochs\"):\n",
    "        total_loss = 0\n",
    "        model.train()\n",
    "        for batch in loader:\n",
    "            opt.zero_grad()\n",
    "            pred = model(batch)\n",
    "            label = batch.y\n",
    "            pred = pred[batch.train_mask]\n",
    "            label = label[batch.train_mask]\n",
    "            loss = model.loss(pred, label)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            total_loss += loss.item() * batch.num_graphs\n",
    "        total_loss /= len(loader.dataset)\n",
    "        losses.append(total_loss)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "          test_acc = test(test_loader, model)\n",
    "          test_accs.append(test_acc)\n",
    "          if test_acc > best_acc:\n",
    "            best_acc = test_acc\n",
    "            best_model = copy.deepcopy(model)\n",
    "        else:\n",
    "          test_accs.append(test_accs[-1])\n",
    "\n",
    "    return test_accs, losses, best_model, best_acc, test_loader\n",
    "\n",
    "def test(loader, test_model, is_validation=False, save_model_preds=False, model_type=None):\n",
    "    test_model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    # Note that Cora is only one graph!\n",
    "    for data in loader:\n",
    "        with torch.no_grad():\n",
    "            # max(dim=1) returns values, indices tuple; only need indices\n",
    "            pred = test_model(data).max(dim=1)[1]\n",
    "            label = data.y\n",
    "\n",
    "        mask = data.val_mask if is_validation else data.test_mask\n",
    "        # node classification: only evaluate on nodes in test set\n",
    "        pred = pred[mask]\n",
    "        label = label[mask]\n",
    "\n",
    "        if save_model_preds:\n",
    "          print (\"Saving Model Predictions for Model Type\", model_type)\n",
    "\n",
    "          data = {}\n",
    "          data['pred'] = pred.view(-1).cpu().detach().numpy()\n",
    "          data['label'] = label.view(-1).cpu().detach().numpy()\n",
    "\n",
    "          df = pd.DataFrame(data=data)\n",
    "          # Save locally as csv\n",
    "          df.to_csv('CORA-Node-' + model_type + '.csv', sep=',', index=False)\n",
    "\n",
    "        correct += pred.eq(label).sum().item()\n",
    "\n",
    "    total = 0\n",
    "    for data in loader.dataset:\n",
    "        total += torch.sum(data.val_mask if is_validation else data.test_mask).item()\n",
    "\n",
    "    return correct / total\n",
    "\n",
    "class objectview(object):\n",
    "    def __init__(self, d):\n",
    "        self.__dict__ = d\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L7-h7jIsCns4"
   },
   "source": [
    "### Training\n",
    "\n",
    "We will be working on the CORA dataset on node-level classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qe9B45l9Cpz2"
   },
   "outputs": [],
   "source": [
    "\n",
    "for args in [\n",
    "    {'model_type': 'GraphSage', 'dataset': 'cora', 'num_layers': 2, 'heads': 1, 'batch_size': 32, 'hidden_dim': 32, 'dropout': 0.5, 'epochs': 500, 'opt': 'adam', 'opt_scheduler': 'none', 'opt_restart': 0, 'weight_decay': 5e-3, 'lr': 0.01},\n",
    "]:\n",
    "    args = objectview(args)\n",
    "    for model in ['GraphSage']:\n",
    "        args.model_type = model\n",
    "\n",
    "        if args.dataset == 'cora':\n",
    "            dataset = Planetoid(root='/tmp/cora', name='Cora')\n",
    "        else:\n",
    "            raise NotImplementedError(\"Unknown dataset\")\n",
    "        test_accs, losses, best_model, best_acc, test_loader = train(dataset, args)\n",
    "\n",
    "        print(\"Maximum test set accuracy: {0}\".format(max(test_accs)))\n",
    "        print(\"Minimum loss: {0}\".format(min(losses)))\n",
    "\n",
    "        # Run test for our best model to save the predictions!\n",
    "        test(test_loader, best_model, is_validation=False, save_model_preds=False, model_type=model)\n",
    "        print()\n",
    "\n",
    "        plt.title(dataset.name)\n",
    "        plt.plot(losses, label=\"training loss\" + \" - \" + args.model_type)\n",
    "        plt.plot(test_accs, label=\"test accuracy\" + \" - \" + args.model_type)\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gHELqjARZ1W5"
   },
   "source": [
    "## Question 1: What is the maximum accuracy obtained on the test set for GraphSage?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Describe the cora dataset. What type of network is it and what classification do we learn? What input features are used for the training?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: What is the dimension of the output of `message`, `aggregate` and `update`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 Implementation of GAT\n",
    "\n",
    "The building block of the Graph Attention Network is the graph attention layer, which is a variant of the aggregation function. Let $N$ be the number of nodes and $F$ be the dimension of the feature vector for each node. The input to each graph attentional layer is a set of node features: $\\mathbf{h} = \\{\\overrightarrow{h_1}, \\overrightarrow{h_2}, \\dots, \\overrightarrow{h_N}$\\}, $\\overrightarrow{h_i} \\in R^F$. The output of each graph attentional layer is a new set of node features, which may have a new dimension $F'$: $\\mathbf{h'} = \\{\\overrightarrow{h_1'}, \\overrightarrow{h_2'}, \\dots, \\overrightarrow{h_N'}\\}$, with $\\overrightarrow{h_i'} \\in \\mathbb{R}^{F'}$.\n",
    "\n",
    "We will now describe how this transformation is performed for each graph attention layer. First, a shared linear transformation parametrized by the weight matrix $\\mathbf{W} \\in \\mathbb{R}^{F' \\times F}$ is applied to every node.\n",
    "\n",
    "Next, we perform self-attention on the nodes. We use a shared attention function $a$:\n",
    "\\begin{equation}\n",
    "a : \\mathbb{R}^{F'} \\times \\mathbb{R}^{F'} \\rightarrow \\mathbb{R}.\n",
    "\\end{equation}\n",
    "\n",
    "that computes the attention coefficients capturing the importance of node $j$'s features to node $i$:\n",
    "\\begin{equation}\n",
    "e_{ij} = a(\\mathbf{W_l}\\overrightarrow{h_i}, \\mathbf{W_r} \\overrightarrow{h_j})\n",
    "\\end{equation}\n",
    "\n",
    "The most general formulation of self-attention allows every node to attend to all other nodes which drops all structural information. However, to utilize graph structure in the attention mechanisms, we use **masked attention**. In masked attention, we only compute attention coefficients $e_{ij}$ for nodes $j \\in N_i$ where $N_i$ is some neighborhood of node $i$ in the graph.\n",
    "\n",
    "To easily compare coefficients across different nodes, we normalize the coefficients across $j$ using a softmax function:\n",
    "\\begin{equation}\n",
    "\\alpha_{ij} = \\text{softmax}_j(e_{ij}) = \\frac{\\exp(e_{ij})}{\\sum_{k \\in N_i} \\exp(e_{ik})}\n",
    "\\end{equation}\n",
    "\n",
    "For this problem, our attention mechanism $a$ will be a single-layer feedforward neural network parametrized by a weight vectors $\\overrightarrow{a_l} \\in \\mathbb{R}^{F'}$ and $\\overrightarrow{a_r} \\in \\mathbb{R}^{F'}$, followed by a LeakyReLU nonlinearity (with negative input slope 0.2). Let $\\cdot^T$ represent transposition and $||$ represent concatenation. The coefficients computed by our attention mechanism may be expressed as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\alpha_{ij} = \\frac{\\exp\\Big(\\text{LeakyReLU}\\Big(\\overrightarrow{a_l}^T \\mathbf{W_l} \\overrightarrow{h_i} + \\overrightarrow{a_r}^T\\mathbf{W_r}\\overrightarrow{h_j}\\Big)\\Big)}{\\sum_{k\\in N_i} \\exp\\Big(\\text{LeakyReLU}\\Big(\\overrightarrow{a_l}^T \\mathbf{W_l} \\overrightarrow{h_i} + \\overrightarrow{a_r}^T\\mathbf{W_r}\\overrightarrow{h_k}\\Big)\\Big)}\n",
    "\\end{equation}\n",
    "\n",
    "For the following questions, we denote `alpha_l` = $\\alpha_l = [...,\\overrightarrow{a_l}^T \\mathbf{W_l} \\overrightarrow{h_i},...] \\in R^n$ and `alpha_r` = $\\alpha_r = [..., \\overrightarrow{a_r}^T \\mathbf{W_r} \\overrightarrow{h_j}, ...] \\in R^n$.\n",
    "\n",
    "\n",
    "At every layer of GAT, after the attention coefficients are computed for that layer, the aggregation function can be computed by a weighted sum of neighborhood messages, where weights are specified by $\\alpha_{ij}$.\n",
    "\n",
    "Now, we use the normalized attention coefficients to compute a linear combination of the features corresponding to them. These aggregated features will serve as the final output features for every node.\n",
    "\n",
    "\\begin{equation}\n",
    "h_i' = \\sum_{j \\in N_i} \\alpha_{ij} \\mathbf{W_r} \\overrightarrow{h_j}.\n",
    "\\end{equation}\n",
    "\n",
    "### Multi-Head Attention\n",
    "To stabilize the learning process of self-attention, we use multi-head attention. To do this we use $K$ independent attention mechanisms, or ``heads`` compute output features as in the above equations. Then, we concatenate these output feature representations:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\overrightarrow{h_i}' = ||_{k=1}^K \\Big(\\sum_{j \\in N_i} \\alpha_{ij}^{(k)} \\mathbf{W_r}^{(k)} \\overrightarrow{h_j}\\Big)\n",
    "\\end{equation}\n",
    "\n",
    "where $||$ is concentation, $\\alpha_{ij}^{(k)}$ are the normalized attention coefficients computed by the $k$-th attention mechanism $(a^k)$, and $\\mathbf{W}^{(k)}$ is the corresponding input linear transformation's weight matrix. Note that for this setting, $\\mathbf{h'} \\in \\mathbb{R}^{KF'}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Not the following hints for the implementation:\n",
    "\n",
    "- Hint 1: Our aggregation is very similar to that of GraphSage except now we are using sum aggregation\n",
    "- Hint 2: The terms we aggregate over again represent the individual message that each neighbor node j sends. Thus, we see that $\\alpha_{ij}$ is part of the message each node sends and is thus computed during the message step. This makes sense since an attention weight is associated with each edge in the graph.\n",
    "- Hint 3: Look at the terms in the definition of $\\alpha_{ij}$. What values do we want to pre-process and pass as parameters to the `propagate` function. The parameters of `message(..., x_j, alpha_j, alpha_i, ...)` should give a good hint.\n",
    "- Hint 4: Note that `update` is missing from the implementation of `propagate`. As a result include the update logic into `propagate` directly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Change `__init__`, `forward`, `message` and `aggregate`\n",
    "class GAT(MessagePassing):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, heads = 2,\n",
    "                 negative_slope = 0.2, dropout = 0., **kwargs):\n",
    "        super(GAT, self).__init__(node_dim=0, **kwargs)\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.heads = heads\n",
    "        self.negative_slope = negative_slope\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.lin_l = None\n",
    "        self.lin_r = None\n",
    "        self.att_l = None\n",
    "        self.att_r = None\n",
    "\n",
    "        ############# Your code here #############\n",
    "        # Define the layers needed for the message functions below.\n",
    "        # self.lin_l is the linear transformation that you apply to embeddings\n",
    "        # before message passing.\n",
    "        # Pay attention to dimensions of the linear layers, since we're using\n",
    "        # multi-head attention.\n",
    "        ############################################################################\n",
    "\n",
    "        self.lin_r = self.lin_l\n",
    "\n",
    "        ############# Your code here #############\n",
    "        # Define the attention parameters \\overrightarrow{a_l/r}^T in the above intro.\n",
    "        # You have to deal with multi-head scenarios.\n",
    "        # Use nn.Parameter instead of nn.Linear\n",
    "\n",
    "        ############################################################################\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform_(self.lin_l.weight)\n",
    "        nn.init.xavier_uniform_(self.lin_r.weight)\n",
    "        nn.init.xavier_uniform_(self.att_l)\n",
    "        nn.init.xavier_uniform_(self.att_r)\n",
    "\n",
    "    def forward(self, x, edge_index, size = None):\n",
    "\n",
    "        H, C = self.heads, self.out_channels\n",
    "\n",
    "        ############# Your code here #############\n",
    "        # Implement message passing, as well as any pre- and post-processing (our update rule).\n",
    "        # 1. First apply linear transformation to node embeddings, and split that\n",
    "        #    into multiple heads. We use the same representations for source and\n",
    "        #    target nodes, but apply different linear weights (W_l and W_r)\n",
    "        # 2. Calculate alpha vectors for central nodes (alpha_l) and neighbor nodes (alpha_r).\n",
    "        # 3. Call propagate function to conduct the message passing.\n",
    "        #    3.1 Remember to pass alpha = (alpha_l, alpha_r) as a parameter.\n",
    "        #    3.2 See there for more information: https://pytorch-geometric.readthedocs.io/en/latest/notes/create_gnn.html\n",
    "        # 4. Transform the output back to the shape of [N, H * C].\n",
    "\n",
    "        out = None\n",
    "\n",
    "        ############################################################################\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "    def message(self, x_j, alpha_j, alpha_i, index, ptr, size_i):\n",
    "\n",
    "        ############# Your code here #############\n",
    "        # Implement your message function.\n",
    "        # 1. Calculate the final attention weights using alpha_i and alpha_j,\n",
    "        #    and apply leaky Relu.\n",
    "        # 2. Calculate softmax over the neighbor nodes for all the nodes. Use\n",
    "        #    torch_geometric.utils.softmax instead of the one in Pytorch.\n",
    "        # 3. Apply dropout to attention weights (alpha).\n",
    "        # 4. Multiply embeddings and attention weights. As a sanity check, the output\n",
    "        #    should be of shape [E, H, C].\n",
    "        # 5. ptr (LongTensor, optional): If given, computes the softmax based on\n",
    "        #    sorted inputs in CSR representation. You can simply pass it to softmax.\n",
    "        # 6. size_i: corresponds to the num_nodes variable input to the torch.geometric.softmax method\n",
    "\n",
    "        out = None\n",
    "\n",
    "        ############################################################################\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "    def aggregate(self, inputs, index, dim_size = None):\n",
    "\n",
    "        ############# Your code here #############\n",
    "        # Implement your aggregate function here.\n",
    "        # See here as how to use torch_scatter.scatter: https://pytorch-scatter.readthedocs.io/en/latest/_modules/torch_scatter/scatter.html\n",
    "        # Pay attention to \"reduce\" parameter is different from that in GraphSage.\n",
    "\n",
    "        out = None\n",
    "        \n",
    "        ############################################################################\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for args in [\n",
    "    {'model_type': 'GAT', 'dataset': 'cora', 'num_layers': 2, 'heads': 1, 'batch_size': 32, 'hidden_dim': 32, 'dropout': 0.5, 'epochs': 500, 'opt': 'adam', 'opt_scheduler': 'none', 'opt_restart': 0, 'weight_decay': 5e-3, 'lr': 0.01},\n",
    "]:\n",
    "    args = objectview(args)\n",
    "    for model in ['GAT']:\n",
    "        args.model_type = model\n",
    "\n",
    "        # Match the dimension.\n",
    "        if model == 'GAT':\n",
    "          args.heads = 2\n",
    "        else:\n",
    "          args.heads = 1\n",
    "\n",
    "        if args.dataset == 'cora':\n",
    "            dataset = Planetoid(root='/tmp/cora', name='Cora')\n",
    "        else:\n",
    "            raise NotImplementedError(\"Unknown dataset\")\n",
    "        test_accs, losses, best_model, best_acc, test_loader = train(dataset, args)\n",
    "\n",
    "        print(\"Maximum test set accuracy: {0}\".format(max(test_accs)))\n",
    "        print(\"Minimum loss: {0}\".format(min(losses)))\n",
    "\n",
    "        # Run test for our best model to save the predictions!\n",
    "        test(test_loader, best_model, is_validation=False, save_model_preds=True, model_type=model)\n",
    "        print()\n",
    "\n",
    "        plt.title(dataset.name)\n",
    "        plt.plot(losses, label=\"training loss\" + \" - \" + args.model_type)\n",
    "        plt.plot(test_accs, label=\"test accuracy\" + \" - \" + args.model_type)\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4: What is the maximum accuracy obtained on test set for GAT?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer 4:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5: What is the resulting accuracy if you increase the head count to 4?\n",
    "\n",
    "Hint: This is done by either modifying `heads` for the current args or adding another args-file. Note, that all other training arguments should not be changed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer 5:"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
